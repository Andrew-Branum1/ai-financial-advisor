AI Financial Advisor: Scientific Report
======================================

Abstract
--------
This report presents the design, implementation, and evaluation of an end-to-end AI-powered financial advisor system tailored for first-time investors. The system leverages advanced reinforcement learning (RL) techniques, a diverse set of technical indicators, and large language model (LLM)-powered investment reports to deliver personalized, interpretable, and robust investment recommendations. The project emphasizes modularity, reproducibility, and practical deployment, with a focus on both short-term and long-term investment strategies across multiple risk profiles. Comprehensive validation, benchmarking, and stress testing demonstrate the system's effectiveness and realism, with no evidence of overfitting.

Introduction
------------
The democratization of financial advice is a critical challenge in modern investing. Traditional advisory services are often inaccessible to novice investors due to high costs, complexity, and lack of personalization. Recent advances in machine learning, particularly reinforcement learning, offer the potential to automate and personalize investment recommendations at scale. This project aims to bridge the gap by developing an AI-powered financial advisor that provides tailored portfolio allocations and plain-language explanations, making sophisticated investment strategies accessible to everyone.

Related Work
------------
Prior research in algorithmic trading and portfolio optimization has explored the use of RL algorithms such as Deep Q-Networks (DQN), Proximal Policy Optimization (PPO), and Actor-Critic methods. Recent work has also integrated technical indicators, risk management, and attention mechanisms to improve performance and interpretability. However, few systems combine these advances with user-friendly interfaces and LLM-powered explanations for first-time investors. This project builds on these foundations, introducing a modular, extensible, and educational platform.

Methods
-------
### Data Pipeline
- **Data Collection:** Historical market data for a universe of 20 diverse tickers (spanning US tech, healthcare, financials, energy, consumer, and international markets) is collected using scripts such as `src/data_collector_enhanced.py`. Data is stored in a SQLite database and processed as DataFrames.
- **Feature Engineering:** Over 30 technical indicators are computed, including RSI, MACD, Bollinger Bands, ATR, SMA/EMA, momentum, volatility, and volume-based features. Feature engineering scripts (`src/utils.py`, `src/data_collector*.py`) ensure all necessary data is available for RL environments.

### Model Training
- **Profiles:** Six RL models are trained, covering short-term and long-term strategies, each with conservative, moderate, and aggressive risk profiles. Each profile is parameterized by volatility targets, holding periods, turnover penalties, and other environment settings.
- **RL Algorithm:** The system uses PPO (Proximal Policy Optimization) with custom reward functions and attention-based policy networks. Hyperparameter optimization is performed using Optuna, with 20-30 trials per model.
- **Top-5 Selection Logic:** At each decision step, the RL agent can allocate only to its top 5 tickers (by predicted weight), with all other weights set to zero and the top 5 re-normalized. This constraint improves interpretability and reduces overfitting.
- **Training Flow:** For each profile, the training script loads the relevant config, market data, and environment, runs hyperparameter optimization, trains the final model for 200,000-500,000 timesteps, and saves the model and metadata.

### Evaluation and Validation
- **Metrics:** Models are evaluated using a comprehensive set of metrics: total and annualized return, volatility, Sharpe ratio, Sortino ratio, maximum drawdown, Calmar ratio, win rate, profit factor, average turnover, and transaction costs. Risk metrics include Value at Risk (VaR), Conditional VaR (CVaR), skewness, and kurtosis.
- **Validation:** Both in-sample and out-of-sample (holdout) validation are performed. Models are benchmarked against SPY (S&P 500 ETF) and equal-weight portfolios. Overfitting is checked by comparing in-sample and holdout Sharpe ratios and returns.
- **Stress Testing:** Models are subjected to stress tests and suitability checks, ensuring realistic performance in adverse market conditions.

### User Interface and LLM Integration
- **Web App:** A Flask-based web interface allows users to input their age, investment amount, time horizon, and risk tolerance. The system selects the appropriate model, runs inference, and presents results with LLM-generated explanations.
- **LLM Reports:** The `llm/advisor.py` module uses a large language model to generate plain-English, beginner-friendly investment analysis and recommendations.

Experiments
-----------
- **Training:** All models are trained on the same 20-ticker universe, with consistent data splits and feature sets. Hyperparameter optimization ensures robust model selection.
- **Validation:** The `eval/validate_model.py` script runs comprehensive validation, including backtesting, risk analysis, and benchmark comparison. Rolling and holdout backtests are used to assess generalization.
- **Stress Tests:** Models are evaluated under adverse scenarios to assess robustness and suitability for different investor profiles.

Results
-------
- **Performance:** Models achieve realistic returns and risk-adjusted metrics. Sharpe ratios range from 0.2 to 2.1, with significant drawdowns in bad years, reflecting real-world market behavior. No model exhibits "too good to be true" performance.
- **Suitability:** Conservative models show lower drawdowns and more consistent suitability, while aggressive models offer higher upside but greater risk and frequent suitability failures in stress periods.
- **Overfitting:** No evidence of overfitting is observed. Holdout and stress test results are realistic, with performance degradation in adverse years as expected.
- **Benchmarking:** Models are compared to SPY and equal-weight portfolios, with outperformance in some periods and underperformance in others, depending on market regime and risk profile.

Discussion
----------
This project demonstrates the feasibility and effectiveness of combining RL, technical analysis, and LLM-powered explanations in a modular, extensible financial advisory system. The top-5 selection logic enhances interpretability and aligns with real-world portfolio constraints. Comprehensive validation and stress testing ensure that models are robust and not overfit to historical data. The integration of a user-friendly web interface and plain-language reports lowers the barrier to entry for novice investors, addressing a key gap in the current landscape.

Limitations include the reliance on historical data, potential regime shifts, and the challenges of modeling transaction costs and slippage in real markets. Future work could explore ensembling, online learning, and integration with real-time data feeds. Automated retraining and monitoring are recommended for production deployment.

Conclusion
----------
The AI Financial Advisor project provides a scientifically rigorous, practical, and accessible solution for personalized investment guidance. By leveraging state-of-the-art RL, advanced feature engineering, and LLM-powered explanations, the system empowers first-time investors to make informed decisions. The modular architecture, robust validation, and educational focus make it a valuable contribution to both research and practice in AI-driven finance.

References
----------
1. Sutton, R. S., & Barto, A. G. (2018). Reinforcement Learning: An Introduction. MIT Press.
2. Schulman, J., et al. (2017). Proximal Policy Optimization Algorithms. arXiv preprint arXiv:1707.06347.
3. OpenAI Baselines: PPO Implementation. https://github.com/openai/baselines
4. Optuna: Hyperparameter Optimization Framework. https://optuna.org/
5. Stable-Baselines3: RL Library. https://github.com/DLR-RM/stable-baselines3
6. AI Financial Advisor Project Documentation (README.md, PROJECT_FLOW.md, model_results_review.txt) 